{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentimentClassificationUsingBert.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMT6RNW01b6pSmP7XC3P5b2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nitin649/SentimentClassificationBERT/blob/main/SentimentClassificationUsingBert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IMMfJoY80Kf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c83171d9-96ca-4ddc-95c2-7aad9fc2c0f5"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 12.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 42.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 50.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 56.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=212d8824f3d3adad01dc0c98bf3928d26324db16bf57b4fe29b7c68afe900b35\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zOc_iE6mYix",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "544d3bb2-70e4-4163-ffbf-7f73e8c717b8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtL_t7tOGucr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9cec555-2c37-4dae-b615-e3f036bb13f9"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from torch.utils import data\n",
        "from pylab import  rcParams\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "import transformers\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from collections import defaultdict\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmZizzmXrM6Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "573a2c3b-d098-4e6d-9230-f72e19058846"
      },
      "source": [
        "!pip install watermark\n",
        "%reload_ext watermark\n",
        "%watermark -v -p numpy,pandas,torch,transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: watermark in /usr/local/lib/python3.6/dist-packages (2.0.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from watermark) (5.5.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->watermark) (50.3.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->watermark) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->watermark) (4.3.3)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->watermark) (2.6.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->watermark) (4.4.2)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->watermark) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->watermark) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->watermark) (0.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->watermark) (1.15.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->watermark) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->watermark) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->watermark) (0.2.5)\n",
            "CPython 3.6.9\n",
            "IPython 5.5.0\n",
            "\n",
            "numpy 1.18.5\n",
            "pandas 1.1.4\n",
            "torch 1.7.0+cu101\n",
            "transformers 3.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lu0tNfuIfVKY"
      },
      "source": [
        "#CUDA_LAUNCH_BLOCKING=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRjNYpA-HV8_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ef45f53-deda-40bf-9bb8-5c2bf2f40ff2"
      },
      "source": [
        "%matplotlib inline\n",
        "sns.set(style='whitegrid',palette='muted',font_scale=1.2)\n",
        "\n",
        "HAPPY_COLOR_PALETTE=['#01BEFE','#FFDD00','#FF7D00','#FF006D','#ADFF02','#8F00FF']\n",
        "\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLOR_PALETTE))\n",
        "\n",
        "rcParams['figure.figsize']=12,8\n",
        "\n",
        "RANDOM_SEED=42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fecb7efa228>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAyMonh6Ix3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7793a89-d9bf-49b3-dac3-15e05b2bcd24"
      },
      "source": [
        "#google app review data downloaded using these 2 drive links\n",
        "!gdown --id 1S6qMioqPJjyBLpLVz4gmRTnJHnjitnuV\n",
        "!gdown --id 1zdmewp7ayS4js4VtrJEHzAheSW-5NBZv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1S6qMioqPJjyBLpLVz4gmRTnJHnjitnuV\n",
            "To: /content/apps.csv\n",
            "100% 134k/134k [00:00<00:00, 48.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1zdmewp7ayS4js4VtrJEHzAheSW-5NBZv\n",
            "To: /content/reviews.csv\n",
            "7.17MB [00:00, 62.7MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wl7VSBZLIo9j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "4e4216d6-c7c4-4b9f-e148-9a56724276b0"
      },
      "source": [
        "df=pd.read_csv('reviews.csv') #loading of data\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userName</th>\n",
              "      <th>userImage</th>\n",
              "      <th>content</th>\n",
              "      <th>score</th>\n",
              "      <th>thumbsUpCount</th>\n",
              "      <th>reviewCreatedVersion</th>\n",
              "      <th>at</th>\n",
              "      <th>replyContent</th>\n",
              "      <th>repliedAt</th>\n",
              "      <th>sortOrder</th>\n",
              "      <th>appId</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Andrew Thomas</td>\n",
              "      <td>https://lh3.googleusercontent.com/a-/AOh14GiHd...</td>\n",
              "      <td>Update: After getting a response from the deve...</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>4.17.0.3</td>\n",
              "      <td>2020-04-05 22:25:57</td>\n",
              "      <td>According to our TOS, and the term you have ag...</td>\n",
              "      <td>2020-04-05 15:10:24</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Craig Haines</td>\n",
              "      <td>https://lh3.googleusercontent.com/-hoe0kwSJgPQ...</td>\n",
              "      <td>Used it for a fair amount of time without any ...</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>4.17.0.3</td>\n",
              "      <td>2020-04-04 13:40:01</td>\n",
              "      <td>It sounds like you logged in with a different ...</td>\n",
              "      <td>2020-04-05 15:11:35</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>steven adkins</td>\n",
              "      <td>https://lh3.googleusercontent.com/a-/AOh14GiXw...</td>\n",
              "      <td>Your app sucks now!!!!! Used to be good but no...</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>4.17.0.3</td>\n",
              "      <td>2020-04-01 16:18:13</td>\n",
              "      <td>This sounds odd! We are not aware of any issue...</td>\n",
              "      <td>2020-04-02 16:05:56</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Lars Panzerbjørn</td>\n",
              "      <td>https://lh3.googleusercontent.com/a-/AOh14Gg-h...</td>\n",
              "      <td>It seems OK, but very basic. Recurring tasks n...</td>\n",
              "      <td>1</td>\n",
              "      <td>192</td>\n",
              "      <td>4.17.0.2</td>\n",
              "      <td>2020-03-12 08:17:34</td>\n",
              "      <td>We do offer this option as part of the Advance...</td>\n",
              "      <td>2020-03-15 06:20:13</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Scott Prewitt</td>\n",
              "      <td>https://lh3.googleusercontent.com/-K-X1-YsVd6U...</td>\n",
              "      <td>Absolutely worthless. This app runs a prohibit...</td>\n",
              "      <td>1</td>\n",
              "      <td>42</td>\n",
              "      <td>4.17.0.2</td>\n",
              "      <td>2020-03-14 17:41:01</td>\n",
              "      <td>We're sorry you feel this way! 90% of the app ...</td>\n",
              "      <td>2020-03-15 23:45:51</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           userName  ...      appId\n",
              "0     Andrew Thomas  ...  com.anydo\n",
              "1      Craig Haines  ...  com.anydo\n",
              "2     steven adkins  ...  com.anydo\n",
              "3  Lars Panzerbjørn  ...  com.anydo\n",
              "4     Scott Prewitt  ...  com.anydo\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qy0IZp_mKR4T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8df690b-5ce6-4deb-d510-0700a21c6a8f"
      },
      "source": [
        "df.shape #shape of the dataset "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15746, 11)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQz35lLPKY3Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba58234a-b3f0-4a24-9942-fc9d25c7abd0"
      },
      "source": [
        "#more information about data\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 15746 entries, 0 to 15745\n",
            "Data columns (total 11 columns):\n",
            " #   Column                Non-Null Count  Dtype \n",
            "---  ------                --------------  ----- \n",
            " 0   userName              15746 non-null  object\n",
            " 1   userImage             15746 non-null  object\n",
            " 2   content               15746 non-null  object\n",
            " 3   score                 15746 non-null  int64 \n",
            " 4   thumbsUpCount         15746 non-null  int64 \n",
            " 5   reviewCreatedVersion  13533 non-null  object\n",
            " 6   at                    15746 non-null  object\n",
            " 7   replyContent          7367 non-null   object\n",
            " 8   repliedAt             7367 non-null   object\n",
            " 9   sortOrder             15746 non-null  object\n",
            " 10  appId                 15746 non-null  object\n",
            "dtypes: int64(2), object(9)\n",
            "memory usage: 1.3+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQJ6Cx7NKhPv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "outputId": "230cddef-1dcb-4b40-d07b-9eadaf0b9a2a"
      },
      "source": [
        "#we can the check the counts of rating for different rate respectively \n",
        "sns.countplot(df.score)\n",
        "plt.xlabel('reviews')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxsonJQKLGrE"
      },
      "source": [
        "#now Group the rating into 2 groups one is good and one is bad and we take rating 3 as threshold so below 3 bad ,above 3 good,and 3 is avg\n",
        "def group_rating(rating):\n",
        "  rating=int(rating)\n",
        "  if rating <=2:\n",
        "    return 0\n",
        "  elif rating==3:\n",
        "    return 1\n",
        "  else:\n",
        "    return 2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9gHhOqyQNc2"
      },
      "source": [
        "df['sentiment']=df.score.apply(group_rating) #converting df scores into our rating"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrkncR5ZQkPx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "outputId": "89ecef95-aa3c-4c7c-ce80-d578b7deb5f9"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userName</th>\n",
              "      <th>userImage</th>\n",
              "      <th>content</th>\n",
              "      <th>score</th>\n",
              "      <th>thumbsUpCount</th>\n",
              "      <th>reviewCreatedVersion</th>\n",
              "      <th>at</th>\n",
              "      <th>replyContent</th>\n",
              "      <th>repliedAt</th>\n",
              "      <th>sortOrder</th>\n",
              "      <th>appId</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Andrew Thomas</td>\n",
              "      <td>https://lh3.googleusercontent.com/a-/AOh14GiHd...</td>\n",
              "      <td>Update: After getting a response from the deve...</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>4.17.0.3</td>\n",
              "      <td>2020-04-05 22:25:57</td>\n",
              "      <td>According to our TOS, and the term you have ag...</td>\n",
              "      <td>2020-04-05 15:10:24</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Craig Haines</td>\n",
              "      <td>https://lh3.googleusercontent.com/-hoe0kwSJgPQ...</td>\n",
              "      <td>Used it for a fair amount of time without any ...</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>4.17.0.3</td>\n",
              "      <td>2020-04-04 13:40:01</td>\n",
              "      <td>It sounds like you logged in with a different ...</td>\n",
              "      <td>2020-04-05 15:11:35</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>steven adkins</td>\n",
              "      <td>https://lh3.googleusercontent.com/a-/AOh14GiXw...</td>\n",
              "      <td>Your app sucks now!!!!! Used to be good but no...</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>4.17.0.3</td>\n",
              "      <td>2020-04-01 16:18:13</td>\n",
              "      <td>This sounds odd! We are not aware of any issue...</td>\n",
              "      <td>2020-04-02 16:05:56</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Lars Panzerbjørn</td>\n",
              "      <td>https://lh3.googleusercontent.com/a-/AOh14Gg-h...</td>\n",
              "      <td>It seems OK, but very basic. Recurring tasks n...</td>\n",
              "      <td>1</td>\n",
              "      <td>192</td>\n",
              "      <td>4.17.0.2</td>\n",
              "      <td>2020-03-12 08:17:34</td>\n",
              "      <td>We do offer this option as part of the Advance...</td>\n",
              "      <td>2020-03-15 06:20:13</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Scott Prewitt</td>\n",
              "      <td>https://lh3.googleusercontent.com/-K-X1-YsVd6U...</td>\n",
              "      <td>Absolutely worthless. This app runs a prohibit...</td>\n",
              "      <td>1</td>\n",
              "      <td>42</td>\n",
              "      <td>4.17.0.2</td>\n",
              "      <td>2020-03-14 17:41:01</td>\n",
              "      <td>We're sorry you feel this way! 90% of the app ...</td>\n",
              "      <td>2020-03-15 23:45:51</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           userName  ... sentiment\n",
              "0     Andrew Thomas  ...         0\n",
              "1      Craig Haines  ...         0\n",
              "2     steven adkins  ...         0\n",
              "3  Lars Panzerbjørn  ...         0\n",
              "4     Scott Prewitt  ...         0\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5GXBK2LQvE0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "outputId": "c19777cc-3358-404a-f510-39c83083d7d1"
      },
      "source": [
        "class_names=['negative','neutral','positive']\n",
        "ax=sns.countplot(df.sentiment)\n",
        "plt.xlabel('review sentiment')\n",
        "ax.set_xticklabels(class_names)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEAI8nv0RVeB"
      },
      "source": [
        "#Data Preprocessing\n",
        "tokenizer=transformers.BertTokenizer.from_pretrained('bert-base-cased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hDr7UFjRwqd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2773b8bb-fe1a-4d84-8530-9076d8b4716e"
      },
      "source": [
        "#choosing the sequence len\n",
        "\n",
        "token_lens=[]\n",
        "\n",
        "for text in df.content:\n",
        "  tokens=tokenizer.encode(text,max_length=512)\n",
        "  token_lens.append(len(tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdhYIgh1RxpF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "17ce8d8c-b811-4c11-ea71-056f3cb5418f"
      },
      "source": [
        "sns.distplot(token_lens);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvUAAAHXCAYAAAA8zqUtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzde3RU9b3//9dMLpOQTIAIBAiSAIohBEIgoKa0ArZcDlU5nuIJVD1tiRWkHGkpP6g9so71eI2LQy1rpZ7it22KiOJRKrRStEWRWkVSLCdcvGC4BQS5ZiaTTJKZ+f0xZGBIApOQuezs52OtrIG9P7P3O9n54zWfvPdnW3w+n08AAAAADMsa7QIAAAAAXB1CPQAAAGBwhHoAAADA4Aj1AAAAgMHFR7sAo/N6vaqtrVVCQoIsFku0ywEAAEAX5fP51NjYqJSUFFmtwXPzhPqrVFtbq08++STaZQAAAMAkhg4dKrvdHrSNUH+VEhISJPl/uImJiVGupn0qKyuVl5cX7TLQQVw/Y+P6GR/X0Ni4fsZm1uvX0NCgTz75JJA/L0aov0rNLTeJiYmy2WxRrqb9jFgzLuD6GRvXz/i4hsbG9TM2M1+/1lq+uVEWAAAAMDhCPQAAAGBwhHoAAADA4Aj1AAAAgMER6gEAAACDI9QDAAAABkeoBwAAAAyOUA8AAAAYHKEeAAAAMDhCPQAAAGBwhHoAAADA4Aj1AAAAgMER6gEAAACDI9QDAAAABkeoBwAAAAyOUA8AAAAYHKEeAAAAMDhCPQAAAGBw8dEuALHhTKNPNZ7QxqbFST0TLOEtCAAAACEj1EOSVOOR/nQ6tLFT0qWeCeGtBwAAAKGj/QYAAAAwOEI9AAAAYHCEegAAAMDgCPUAAACAwRHqAQAAAIMj1AMAAAAGR6gHAAAADI5QDwAAABgcoR4AAAAwOEI9AAAAYHCEegAAAMDgCPUAAACAwRHqAQAAAIMj1AMAAAAGR6gHAAAADI5QDwAAABgcoR4AAAAwOEI9AAAAYHCEegAAAMDgCPUAAACAwRHqAQAAAIMj1AMAAAAGR6gHAAAADI5QDwAAABgcoR4AAAAwuIiGeq/Xq+XLl6uoqEgFBQWaM2eOqqur2xy/Z88eFRcXKz8/XxMmTFB5eXnQ/vLycs2cOVP5+fmaNGlSi/f/8pe/VEFBQdBXTk6O5s2bFxizdOlSDR8+PGhMaWlp533TAAAAQJjFR/Jkq1at0saNG7V69WplZGToySef1Ny5c/X73/9eVmvw5wun06mSkhLNnj1bv/3tb7V37159//vfV58+fTR16lRJUp8+fVRSUqLPP/9c69ata3G+uXPnau7cuYH/nz59WrfccovuuOOOoHG33XabnnzyyTB8xwAAAED4RXSmfu3atSopKdHgwYOVkpKixYsXq6qqShUVFS3Gbt68WVarVQ888IBsNptGjRqlmTNnas2aNYExU6dO1ZQpU5SRkRHS+V955RX16NFDX//61zvtewIAAACiLWIz9Q6HQ9XV1crLywtsS0tLU1ZWlvbu3auxY8cGjd+3b59yc3ODZvDz8vJanZEPhdfr1dq1a/Wv//qvio8P/rbfeust3XjjjbLb7SoqKtLChQuVnp7eruNXVlZ2qK5oa/5A1ZjeTwe/aAjpPcfciTp5+lg4y0KIWvtADOPg+hkf19DYuH7GxvULFrFQ73Q6JfmD/MXsdntg36Xj7XZ70La0tLRWx4binXfe0fHjx3XXXXcFbb/77ru1aNEi9erVS4cPH9YjjzyiefPmae3atbJYLCEfPy8vTzabrUO1RUtFRYXGjBkjSTpY71NWiOX3S5eyBvUPY2UIxcXXD8bD9TM+rqGxcf2MzazXz+12tzmRHLH2m9TUVEn+GfuLORyOwL5Lx18a4GtqalodG4o1a9bo1ltvVZ8+fYK25+XlqXfv3rJYLBo4cKAee+wxffTRRzpw4ECHzgMAAABEWsRCvd1uV2ZmZtCnC4fDoUOHDmnYsGEtxufk5GjPnj3yer2Bbbt371ZOTk67z3348GFt27ZNs2fPvuLY5tl5n8/X7vMAAAAA0RDRG2WLi4v1/PPPq6qqSi6XS6WlpcrOzm71zyeTJ0+Wx+NRWVmZGhoatGvXLq1bt06zZs0KjGlqapLb7VZTU5N8Pp/cbrfcbneLQP7iiy9q0KBBuummm4K2u91ubdq0KfDXg+rqaj388MMaPny4srOzO/8HAAAAAIRBREN9SUmJpk2bptmzZ6uoqEjV1dUqKyuT1WrVjh07VFBQoKNHj0ryt9+sWrVKW7duVWFhoRYsWKD58+dr2rRpgeOVlZVp5MiRevjhh3X06FGNHDlSI0eODFr7vqGhQa+++mrQh4FmXq9X5eXluvXWWzVq1Cjdfffd6tu3r5577rkWS2wCAAAAsSqi69RbrVYtWrRIixYtarGvsLBQO3fuDNqWm5url156qc3jLViwQAsWLLjsORMTE/X++++3ui85OTloiUwAAADAiJiOBgAAAAyOUA8AAAAYHKEeAAAAMDhCPQAAAGBwhHoAAADA4Aj1AAAAgMER6gEAAACDI9QDAAAABkeoBwAAAAyOUA8AAAAYHKEeAAAAMDhCPQAAAGBwhHoAAADA4Aj1AAAAgMER6gEAAACDI9QDAAAABkeoBwAAAAyOUA8AAAAYHKEeAAAAMDhCPQAAAGBwhHoAAADA4Aj1AAAAgMER6gEAAACDI9QDAAAABkeoBwAAAAyOUA8AAAAYHKEeAAAAMDhCPQAAAGBwhHoAAADA4Aj1AAAAgMER6gEAAACDI9QDAAAABkeoBwAAAAyOUA8AAAAYHKEeAAAAMDhCPQAAAGBwhHoAAADA4Aj1AAAAgMER6gEAAACDI9QDAAAABkeoR7vtckrvnPFFuwwAAACcFx/tAmAcjV7p9ZPSW2ckm1X6+Eafrk2yRLssAAAA02OmHiGpdkuPH5TePCPN6CX5JP3082hXBQAAAIlQjxCtOyHVeKQFA6Tl10s/HCCtPi59WEMbDgAAQLQR6hGSareUnyoNT/H/f2mW1CdB+vFnks9HsAcAAIimiIZ6r9er5cuXq6ioSAUFBZozZ46qq6vbHL9nzx4VFxcrPz9fEyZMUHl5edD+8vJyzZw5U/n5+Zo0aVKL9x85ckQ33HCDRo0apYKCgsCXw+EIjKmvr9eyZcs0btw4jR49WgsXLtTZs2c775vuApxNksMj9Uu8sC0t3qL/HCS9e05afzJ6tQEAACDCoX7VqlXauHGjVq9erW3btql///6aO3euvF5vi7FOp1MlJSUaP368tm/frhUrVmjlypXatGlTYEyfPn1UUlKiuXPnXva8Gzdu1M6dOwNfdrs9sO/xxx9XZWWlNmzYoC1btsjlcmnJkiWd9013Acca/K8Xh3pJKukn5XaTHvpc8jJbDwAAEDURDfVr165VSUmJBg8erJSUFC1evFhVVVWqqKhoMXbz5s2yWq164IEHZLPZNGrUKM2cOVNr1qwJjJk6daqmTJmijIyMDtVTX1+v9evX68EHH1RGRoa6d++uJUuW6O2339bRo0c7/H12Nc2hvr8teHu81aKfZEkfu6S3+eMGAABA1ERsSUuHw6Hq6mrl5eUFtqWlpSkrK0t79+7V2LFjg8bv27dPubm5slovfO7Iy8vTunXr2n3uWbNmqaGhQYMGDdKcOXP0jW98Q5J04MABud1ujRgxIjB2yJAhSk5O1t69e9W/f/+Qz1FZWdnuumJB8weqxvR+OvhFQ6tjPm7sqUSlqqb6sBwW6Zg7USdPH5MkDfJZlKYReqqyRt27HYhU2TivtQ/EMA6un/FxDY2N62dsXL9gEQv1TqdTkj/IX8xutwf2XTr+4jaZ5ve2NrYtPXv21Nq1azV8+HB5vV69+eab+tGPfqSVK1fqlltuCRzr0vO0VdPl5OXlyWazXXlgDKmoqNCYMWMkSQfrfcpqo/zaw1I/r5SdlSVJ6pcuZQ268IHnO5/69MvqdGWNSFevRNatj5SLrx+Mh+tnfFxDY+P6GZtZr5/b7W5zIjli7TepqamSFHSTavP/m/ddOv7SYF1TU9Pq2LakpKSooKBAiYmJSkpK0m233abbb79dr7/+eodqMqtj7pb99Be7r7/U4JPKv4hcTQAAALggYqHebrcrMzMz6NOFw+HQoUOHNGzYsBbjc3JytGfPnqCbaHfv3q2cnJyrqsNqtQaWYMzOzpbNZguqaf/+/aqrq7vq83QVtR7pnKdlP/3FhqdYdHOatOoYy1sCAABEQ0RvlC0uLtbzzz+vqqoquVwulZaWKjs7u9U/n0yePFkej0dlZWVqaGjQrl27tG7dOs2aNSswpqmpSW63W01NTfL5fHK73XK73YFguWPHDn322WdqampSQ0OD/vjHP+r3v/+9pk+fLklKSkrSjBkz9Oyzz+rEiRM6d+6cSktLdcsttygzMzMyP5QY90UbK99cqqS/tM8l/fVc+GsCAABAsIiG+pKSEk2bNk2zZ89WUVGRqqurVVZWJqvVqh07dqigoCCw6kxqaqpWrVqlrVu3qrCwUAsWLND8+fM1bdq0wPHKyso0cuRIPfzwwzp69KhGjhypkSNHBta+r6qq0ty5c1VYWKjx48frN7/5jZ566indeuutgWM89NBDGjZsmKZPn66JEyfKZrPp6aefjuSPJaYddftfrxTq7+ojpcX5Z+sBAAAQWRYf/RJXpfmGha5wo+yfTrcc8/IJadtZacX1kvX8PbBT0qWspJY3xM772KfffiEd+4rUPZ4bZsPNrDcJdRVcP+PjGhob18/YzHr9Lpc7IzpTD+P5wi31TbwQ6C/nu/2keq/0yonw1wUAAIALCPW4rGMNUr8Q/wAx1i7d0E36HavgAAAARBShHm2q80hnmq7cT9/MYrHo3r7S1nNSVR1dXQAAAJFCqEebjjWvfNOOWwW+nSFZxGw9AABAJBHq0abmUN8/xJl6SRqYZNHEHtLvjrNmPQAAQKQQ6tGmY24pwSJdk9C+993bT9pfJ/2tJjx1AQAAIBihHm061hD6yjcXu7OX1M0q/ZYWHAAAgIiIj3YBiF3nmqT0Vmbpm7z+de0vZ0q6tPa4tOhan3onSD0TWLceAAAgXAj1aFOtRxrYyk2ytR7p/bOXf29WkuTwSM8ckn6aLfVsZwsPAAAAQkf7DdpU65G6xXXsvUO7ST3jpQ/oqwcAAAg7Qj1a1eiVGnxSagdDvdUijUuTdtdKXzZ0bm0AAAAIRqhHq2q9/teOztRL0k1pklfS6yc7pSQAAAC0gVCPVtV6/K8pVxHq+9n8vfWvftk5NQEAAKB1hHq0KhDqr/I35KY0aY9L2uXkQVQAAADhQqhHqzpjpl6Sxtr9D7AqZ816AACAsCHUo1WuTgr1qfHSxB7SmuNSk5fZegAAgHAg1KNVzk4K9ZI0o7f0RYP0zhXWtgcAAEDHEOrRKpdXirdIiZ3wINiJPfwfDl7mhlkAAICwINSjVbUeqZtVsnRCqE+Kk26/xr8KDi04AAAAnY9Qj1bVejr+4KnWzOwjnWqU/kILDgAAQKcj1KNVtZ6re/DUpaamS/Y46eUTnXdMAAAA+BHq0apab+fcJNssKc6iO3pJr30pNdKCAwAA0KkI9WhVradzQ73kb8E50yS9daZzjwsAAGB2hHq0qtZz9U+TvdTkdCktTlpHCw4AAECnItSjhQav1Ojr/Jl6m9WiGb2l105KblpwAAAAOg2hHi3UduKDpy71L72lc03SVlbBAQAA6DSEerTg8vpfO3P1m2aTeko2q/TGqc4/NgAAgFkR6tGC8/xMfWeuU98sJc6iCT2kN053/rEBAADMKj7aBSD2uJrbbzrpI1+TVzpYf6GH/sY06U+npXfP+jQwKXhsWpzUM6ETHmMLAABgIoR6tNDcU99Z7Te1Hun9i3ro485n9rJqaULP4LFT0qWeCZ1zXgAAALOg/QYt1Iax/UaSMhKl3glSZW14jg8AAGA2hHq0UOuV4i1SOLtg8lKkj13+5TMBAABwdQj1aKH5wVOWcIb6VP9a+J/Whe8cAAAAZkGoRwu1nvCsUX+x65P9fwmodIb3PAAAAGZAqEcLkQj1iVbphm701QMAAHQGQj1acHnC8+CpS+WlSF82Sicawn8uAACAroxQjxac3vCtfHOxnBT/6yeu8J8LAACgKyPUowWXR+oWgd+MjATJHsfNsgAAAFeLUI8gDV7/qjTh7qmX/KvrXJ8sfcpMPQAAwFUh1CNI84OnIhHqJen6btLpJulUY2TOBwAA0BUR6hEk4qE+2f/6GbP1AAAAHUaoR5Da8094jVSo72/z9+/TVw8AANBxhHoECczUR+g3w2qRhiQT6gEAAK4GoR5BIt1+I/n76o83SDVNkTsnAABAV0KoR5CohPrzffXM1gMAAHRMREO91+vV8uXLVVRUpIKCAs2ZM0fV1dVtjt+zZ4+Ki4uVn5+vCRMmqLy8PGh/eXm5Zs6cqfz8fE2aNKnF+9955x3de++9uvHGGzV27FjNmjVL27dvDxqzdOlSDR8+XAUFBYGv0tLSzvmGDajWIyVYpMQI/mYMTJISLdwsCwAA0FERDfWrVq3Sxo0btXr1am3btk39+/fX3Llz5fV6W4x1Op0qKSnR+PHjtX37dq1YsUIrV67Upk2bAmP69OmjkpISzZ07t9XznTt3Tvfcc4/eeust/e1vf9P06dP1/e9/X8eOHQsad9ttt2nnzp2Br8WLF3fuN24gLq/ULYKz9JIUZ5EG01cPAADQYREN9WvXrlVJSYkGDx6slJQULV68WFVVVaqoqGgxdvPmzbJarXrggQdks9k0atQozZw5U2vWrAmMmTp1qqZMmaKMjIxWz3f77bfrG9/4hux2u+Lj43X33XfLZrPp//7v/8L2PRqd0yOlRqEp6/pkqdotnaOvHgAAoN3iI3Uih8Oh6upq5eXlBbalpaUpKytLe/fu1dixY4PG79u3T7m5ubJaLyTMvLw8rVu3rsM17N27Vw6HQ0OHDg3a/tZbb+nGG2+U3W5XUVGRFi5cqPT09HYdu7KyssN1RVPzB6rG9H46+EWDTrszFCfp4MHjbb4nLztdBw+eDvkcoYy3e2zyqa/ePHhajfUHQj622bX2gRjGwfUzPq6hsXH9jI3rFyxiod7pdEryB/mL2e32wL5Lx9vt9qBtaWlprY4NxZdffql///d/1/e+9z1lZ2cHtt99991atGiRevXqpcOHD+uRRx7RvHnztHbtWlkslpCPn5eXJ5vN1qHaoqWiokJjxoyRJB2s9ynLJnmqpN6JUlZmVpvvS02VsrLsbe7vyPi+XunFT6WDCekaM/yakI9tZhdfPxgP18/4uIbGxvUzNrNeP7fb3eZEcsQaLVJTUyX5Z+wv5nA4AvsuHX9pgK+pqWl17JUcP35c99xzj7761a9q0aJFQfvy8vLUu3dvWSwWDRw4UI899pg++ugjHThwoN3n6QrcPik5Cu03NquUaZM+6thnNgAAAFOLWHyz2+3KzMwM+nThcDh06NAhDRs2rMX4nJwc7dmzJ+gm2t27dysnJ6dd5z18+LBmz56tiRMnatmyZVecfW/e7/P52nWersLt9QfsaBiUJP3DKXlN+rMHAADoqIjGt+LiYj3//POqqqqSy+VSaWmpsrOzW/3zyeTJk+XxeFRWVqaGhgbt2rVL69at06xZswJjmpqa5Ha71dTUJJ/PJ7fbLbfbHQjk+/fv17e//W3dfvvtWrJkSYtzuN1ubdq0KfDXg+rqaj388MMaPnx4UIuOmdRHM9QnSw6PtI+lLQEAANolovGtpKRE06ZN0+zZs1VUVKTq6mqVlZXJarVqx44dKigo0NGjRyX5229WrVqlrVu3qrCwUAsWLND8+fM1bdq0wPHKyso0cuRIPfzwwzp69KhGjhypkSNHBta+X7VqlY4fP67f/OY3QevQ//KXv5TkXze/vLxct956q0aNGqW7775bffv21XPPPRd0g65ZeHxSk0+yhX4rQacanOR/fb8mOucHAAAwqojdKCtJVqtVixYtatHXLkmFhYXauXNn0Lbc3Fy99NJLbR5vwYIFWrBgQZv7n3jiCT3xxBNt7k9OTg5aItPs3Oc7nZKi9HmmT6Jkj5M+qJG+1y86NQAAABiR+aaj0abmUB+t9hurRRqVKn1wLjrnBwAAMCpCPQKiHeolqcAuVdZKziZulgUAAAgVoR4B7vM5OpqhflSq5JW0w3HFoQAAADiPUI+A+hiYqR91/jEE3CwLAAAQOkI9AqJ9o6wk9UyQrk/23ywLAACA0BDqERDoqY/SkpbNbkrzh3qzPgAMAACgvQj1CIiFG2UlaVya9EWDdMgd3ToAAACMglCPgFgJ9Td197++z9KWAAAAISHUIyAWVr+RpJEp/r5++uoBAABCQ6hHQL1XirdIcVHuqU+wWjTGTqgHAAAIFaEeAW5vdFe+udiNadLfnZLby82yAAAAVxIjEQ6xwO2NfutNs5vS/PX8wxntSgAAAGJfjEQ4xAK3N/rLWTa7Kc3/SgsOAADAlRHqERBLM/UDkizqn0ioBwAACEWMRDjEgvoYCvWSf2lLlrUEAAC4shiKcIg2ty+2Qv2NadLn9dKXDdwsCwAAcDkxFOEQbbG0+o3kD/USLTgAAABXEkMRDtEWSzfKStIYu3/N/PcJ9QAAAJdFqEdALN0oK0kpcRaNTJG2E+oBAAAuK4YiHKLJ45MaYqynXvK34HxQI3l89NUDAAC0JcYiHKKlzut/jcVQ7/BI+1zRrgQAACB2xViEQ7S4PP7XWAv1N3X3v7K0JQAAQNtiLMIhWpznQ30srX4jSdcnSz3jWQEHAADgcmIswiFaYnWm3mqxaFwaoR4AAOByYizCIVpqm3vqY2hJy2Y3pkmVtZKjiZtlAQAAWkOoh6TYnamXpJvSJJ+kHY5oVwIAABCb2hXhKioqwlUHoqw2hkP9uPNPluUhVAAAAK1rV4S7++679c1vflPl5eU6d47lSLoS1/n2m1i7UVaS0hMsGppMXz0AAEBb2hXh3nzzTX3961/X888/r6997Wv68Y9/rA8//DBctSGCYnmmXvIvbflBjeTjIVQAAAAttCvCDRgwQAsXLtSWLVu0YsUKuVwuffe739XUqVP161//WqdPnw5XnQizWO6pl/w3yx5vkA7WR7sSAACA2NOhCGe1WjVx4kT94he/0E9+8hNVV1frqaee0oQJE/Qf//EfOnPmTGfXiTBzeaU4SfExuPqN5A/1En31AAAArYnvyJsOHTqkdevW6bXXXpPb7dbMmTN111136fjx41q5cqXmz5+vNWvWdHatCKNaT2zM0jd5pYP1LVts0uL8/f5vnZFu7u4L2t4zIUY/iQAAAERIu0L9H/7wB7388svavn27hg8froULF2r69OlKTk6WJOXk5GjIkCGaMmVKWIpF+LhiJNTXeqT3z7a+b4BN2nLmwmo4kjQlXeqZEJnaAAAAYlW7Qv3DDz+sb37zm1qyZIlyc3NbHdOrVy/NmzevU4pD5NR6YyPUX86gJGnLWanRKyXEeK0AAACR1K5Q/+677yolJeWyY5KSkvSDH/zgqopC5NV6YnM5y4sNSpbePCMdcfv/DQAAAL92xbjCwkKdOnWqxfYzZ85o2LBhnVYUIi9WeuovZ1CS/7WKFXAAAACCtCvGtbVGeGNjo+Li4jqlIERHrPTUX07PBKlnvFRVF+1KAAAAYktI7Tfr16+XJFksFr3xxhtKTU0N7PN4PPrggw+UlZUVngoRES6v1NsAn8uyk5ipBwAAuFRIof6nP/1p4N9PPPFE0L6EhAQNGDBAS5cu7dzKEFG1HulaW7SruLJBydJOp1TTJKV1aEFWAACAriekWLR7925J0qRJk/TKK68oPT09rEUh8ozQfiNJg8/31R+ol0amXn4sAACAWbQrxv3lL38h0HdBXp9PLgMsaSlJA5P8v7Sf01cPAAAQcMWZ+g0bNmjKlClKTEzUhg0bLjv2tttu67TCEDl1XsknY4T6RKv/IVQH6KsHAAAIuGKoX7x4sYqKinTNNddo8eLFbY6zWCyEeoNyevyvNkt06wjVoGTpgxrJ2/piTAAAAKZzxVC/b9++Vv+NriMQ6g0wUy/516t/56x0rCHalQAAAMQGg8Q4hJPhQv35p8myXj0AAIBfu2LcX//6V1VUVAT+//LLL+tf/uVftHTpUjmdziu+3+v1avny5SoqKlJBQYHmzJmj6urqNsfv2bNHxcXFys/P14QJE1ReXh60v7y8XDNnzlR+fr4mTZrU6jHee+893X777crPz9eUKVP0xz/+MWj/mTNntHDhQo0ePVrjxo3TsmXL1NBgrilgZ5P/1Sihvk+ClGJlvXoAAIBm7YpxpaWlOnPmjCSpqqpKP/vZz5SXl6fKyko9/fTTV3z/qlWrtHHjRq1evVrbtm1T//79NXfuXHm93hZjnU6nSkpKNH78eG3fvl0rVqzQypUrtWnTpsCYPn36qKSkRHPnzm31fEeOHNG8efN0zz336MMPP9TSpUv1k5/8RP/4xz8CY3784x/L5XJpy5Yt2rBhgyorK/Xkk0+258dieI7zM/VJBgn1FouUncxMPQAAQLN2xbhDhw5p6NChkqQ333xTN998sx555BE9+uijevvtt6/4/rVr16qkpESDBw9WSkqKFi9erKqqqqDZ/2abN2+W1WrVAw88IJvNplGjRmnmzJlas2ZNYMzUqVM1ZcoUZWRktHq+1157TUOHDtXMmTOVmJioiRMnauLEiVq7dq0kf+jftm2blixZou7duysjI0MPPvigXn31Vbnd7vb8aAzNaO03kn+9+mMNkqMp2pUAAABEX7ufyWmx+JdI+fDDD/WVr3xFkpSRkaGzZ89e9n0Oh0PV1dXKy8sLbEtLS1NWVpb27t2rsWPHBo3ft2+fcnNzZbVeSJp5eXlat25dyLXu27cv6HzNx/jDH/4Q2J+cnKwhQ4YE9o8YMUJ1dXWqqqpSTk5OyOeqrKwMeWwsqaioUGVDuqRsnTx6RE1WzxXfk5edroMHT4d8jvaMD3VsN0+SfMrQ24dOyV13MORauprWPhDDOLh+xsc1NDaun7Fx/YK1K9TfcMMNWrNmjfDL4gYAACAASURBVCZNmqT3339fS5YskSQdO3bsig+lau65T0tLC9put9tb7cd3Op2y2+1B29LS0kLq3b/4GNddd12bx2jtHM3/b895JP+HBZvN1q73RFtFRYXGjBmjvx3xSZ9KQ64doNQQfiNSU6WsLPuVB3ZgfKhje3mklz6TDiVeox/k9gq5lq6k+frBmLh+xsc1NDaun7GZ9fq53e42J5Lb1XCxaNEivfrqq7r33ns1Y8aMQGDesmWLRowYcdn3pqamSvLP2F/M4XAE9l06/tJgXVNT0+rYy53z0vNdfIzWztE8vj3nMTojtt+kxEkZidJH7fvsBQAA0CW1a6a+sLBQ7733nmpra4Nm3O+66y4lJydf9r12u12ZmZmqrKwMfABwOBw6dOiQhg0b1mJ8Tk6O3njjDXm93kALzu7du9vVEpOTk6N33303aNvFx8jJyZHL5dL+/fsDLTiVlZVKSkrSoEGDQj6P0Tk9UpykeIM8fKrZoCR/qPf5fIG2MAAAADNq99xsXFxcixaagQMHqnfv3ld8b3FxsZ5//nlVVVXJ5XKptLRU2dnZrf75ZPLkyfJ4PCorK1NDQ4N27dqldevWadasWYExTU1Ncrvdampqks/nk9vtltvtls/nf9TojBkz9PHHH+t///d/1djYqHfeeUdbtmxRcXGxJGnAgAEaP368SktLde7cOZ04cULPPvus7rzzTsO10lwNp0fqFudfVcZIspOkk43SIfPc0wwAANCqds3Ue71evfbaa3rvvfd06tSpFktRXrqO/KVKSkrkcDg0e/Zs1dXVacyYMSorK5PVatWOHTt033336Q9/+IP69++v1NRUrVq1So888oiee+459ezZU/Pnz9e0adMCxysrK9PKlSsD/x85cqQk6c9//rMGDBiga6+9VmVlZXriiSf0yCOPqG/fvnr88ceVn58feE9paakeeeQRTZw4UXFxcZo2bZqWLl3anh+L4Tk9/nYWo2l+CNUHNVJWUnRrAQAAiKZ2hfqnnnpKL7zwgm6++WZlZma2u+XBarVq0aJFWrRoUYt9hYWF2rlzZ9C23NxcvfTSS20eb8GCBVqwYMFlz1lUVKQNGza0uT89PV0///nPr1B511brkboZqJ++WaZNSrT4Q/1dfaJdDQAAQPS0K9Rv3LhRzzzzjKZOnRquehAFRp2pj7dIeSnShzXRrgQAACC62jU/29TUpNzc3HDVgihp7qk3olF2qcIhNXp90S4FAAAgatoV6m+//Xb96U9/ClctiBKjztRL0qhUqc4rVdZGuxIAAIDoaVf7jd1u16pVq7Rz504NGzZMCQkJQfvnzp3bqcUhMpweqX9itKvomFHnHyfwQY1UEPrzsAAAALqUdoX69evXKyUlRfv27dO+ffuC9lksFkK9QTmapGSDztQPsEm9E6TtNdLczGhXAwAAEB3tCvV/+ctfwlUHosjlNebqN5J/bf1xaf6ZegAAALPqcJQ7c+ZM4CFPMLZaA98oK/lD/T6XdK6J30cAAGBO7Qr1Ho9Hzz77rMaOHauvfOUrOnLkiCTpmWee0dq1a8NSIMKr0etTo09KNuhMvSTdmCb5JO1gth4AAJhUu6Lcr371K61fv14//elPg26Szc3N1WuvvdbpxSH8aj3+VyOH+nHnb5B9n1APAABMql1R7rXXXtN//ud/asaMGbJaL7x16NChOnDgQGfXhgio9fpfjXqjrCT1SLAopxt99QAAwLzaFeqPHTumIUOGtNgeFxen+vr6TisKkeM6P1Nv1Btlm92U5p+p5z4PAABgRu2KcpmZmS2WspSk9957T4MHD+60ohA5ze03Rr5RVpJu7i6dbJQ+q4t2JQAAAJHXriUtZ8+erccee0w2m02SdODAAW3dulX//d//raVLl4alQIRXc6hPsvqfzGpUN6f5X/9WI13fLbq1AAAARFq7Qv0999yjs2fP6gc/+IHq6+t13333yWaz6f7779e3vvWtcNWIMGruqe9m8FCfmyKlxUnvnZPu7RvtagAAACKrXaFekhYsWKBvfetbOnnypHw+n6677jp168bUqFFd3H5zqim6tVwNq8WiG9N8ev9ctCsBAACIvJBD/enTp/XMM8/ozTfflNPplCTZ7XZNnjxZP/rRj5Senh62IhE+ri6wpGWzm7tL/3VAcjT5ZI+3RLscAACAiAkp1NfV1Wn27Nk6c+aM7rjjDl133XXy+Xz69NNPtXHjRv3973/Xq6++qqSkpHDXi04WaL8x+I2ykr+v3itpe410K58xAQCAiYQU6l944QW53W69/vrrysjICNp3//33q7i4WGvWrNH3vve9sBSJ8OkKD59qduNFN8sS6gEAgJmEFOX+8pe/6P77728R6CUpIyND9913n/785z93enEIv64U6nskWJTbTfobffUAAMBkQopyn3/+ucaMGdPm/sLCQu3fv7/TikLkuDxSgkVK6AKhXpJu6u5/CJWXh1ABAAATCSnKOZ1O9ejRo839PXr0CNw8C2Op9UopXaCfvtnNadKZJukTV7QrAQAAiJyQQr3H41FcXNvJz2q1yuPxdFpRiJxaT9cK9UXd/a9/q4luHQAAAJEU0o2yPp9PDz74oBISElrd39jY2KlFIXJcHv+Dp7qKG7pJPeOlv56Tvtsv2tUAAABERkih/p//+Z+vOGbAgAFXXQwir6vN1FstFo3v7tO7Z6NdCQAAQOSEFOqfeOKJcNeBKHEZvKe+ySsdrA++KXZEqrThlPTBOZ/62i5sT4uTeibwUCoAAND1hPxEWXRNtR5/2DWqWo/0/iWz8s0R/1fHpHFpF7ZPSZd6tt5BBgAAYGhdqJsaHVHr6RpPk73YtTYpySp9ygo4AADAJAj1JtfVeuolyWqRrk9mWUsAAGAehHqTq/V2vZl6Sbq+m3S8UTrXFO1KAAAAwo9Qb3Iuj5TSBX8Lbujmf2W2HgAAmEEXjHMIlc/XNdtvJGnA+b56Qj0AADADQr2JNcgir7pm+02cRbouWfqkLtqVAAAAhB+h3sTqfP7L3xVn6iVpaDfpeAN99QAAoOsj1JtY/fnL3xV76iX/CjgSS1sCAICur4vGOYSivovP1A9MkmwW6WNCPQAA6OII9SZWd/7yd8WeesnfVz+0m7TX5b8pGAAAoKsi1JtYnc+f5rtq+40kDU+RTjZKJxqjXQkAAED4dOE4hyvp6jfKSv5QL0m7a6NbBwAAQDgR6k3Mra4f6nsnShkJhHoAANC1EepNzAwz9ZKUm+J/CFW9J9qVAAAAhAeh3sQCN8p28d+CvFSp0Sd9UBPtSgAAAMKji8c5XI5ZZuqvT5YSLNLbZ6NdCQAAQHgQ6k2svosvadks0epf2vIdQj0AAOiiCPUmVu+zKskqxVks0S4l7IanSFX10ud1LFgPAAC6HkK9idX5rF2+9aZZ89KWm05Htw4AAIBwiGio93q9Wr58uYqKilRQUKA5c+aourq6zfF79uxRcXGx8vPzNWHCBJWXlwftr6+v17JlyzRu3DiNHj1aCxcu1NmzF3osli1bpoKCgqCvG264Qf/1X/8VGHPPPfcoLy8vaMwLL7zQ+d98DKpTXJe/SbZZnwRpoE1641S0KwEAAOh8EY10q1at0saNG7V69Wpt27ZN/fv319y5c+X1eluMdTqdKikp0fjx47V9+3atWLFCK1eu1KZNmwJjHn/8cVVWVmrDhg3asmWLXC6XlixZEtj/s5/9TDt37gx8vfjii5KkO+64I+hc999/f9C4b3/722H6CcSWehPN1Fss0q09pbfOSM4mWnAAAEDXEtFQv3btWpWUlGjw4MFKSUnR4sWLVVVVpYqKihZjN2/eLKvVqgceeEA2m02jRo3SzJkztWbNGkn+Wfr169frwQcfVEZGhrp3764lS5bo7bff1tGjR1s9/5o1azRy5EiNGDEirN+nUdTLPKFekianS26vtPlMtCsBAADoXPGROpHD4VB1dbXy8vIC29LS0pSVlaW9e/dq7NixQeP37dun3NxcWa0XPnfk5eVp3bp1kqQDBw7I7XYHBfQhQ4YoOTlZe/fuVf/+/YOO53Q6tWHDBi1btqxFbatXr1Z5ebmuueYaff3rX9e8efOUkpLSru+vsrKyXeNjQZ3venldDlVUfKrG9H46+EVDSO/Ly07XwYOhN6e3Z3w4j31Hdg+lWe1aVVWv/s5zVxyf7G1S09kvQ64lGlr7QAzj4PoZH9fQ2Lh+xsb1CxaxUO90OiX5g/zF7HZ7YN+l4+12e9C2tLS0wNjm10vHtHW89evXKzExUf/0T/8UtP2HP/yhBg8erLS0NH3yySd66KGHdOTIEa1YsaJd319eXp5sNlu73hNtde84NbBHisbkj9HBep+yQiw/NVXKyrJfeWAHxofz2EmpUm6qtNXZTTsTuynuCov+TEmXsoYMDLmWSKuoqNCYMWOiXQY6iOtnfFxDY+P6GZtZr5/b7W5zIjli7TepqamS/DP2F3M4HIF9l46/NJzX1NQExrb3eC+++KLuvPPOFsF79OjR6tGjh6xWq3JycvTQQw9p8+bNqq+vb+d3aDx1Jmu/kaT8VMnllT51RbsSAACAzhOxUG+325WZmRn06cLhcOjQoUMaNmxYi/E5OTnas2dP0E20u3fvVk5OjiQpOztbNpst6Hj79+9XXV1dYEyzDz74QPv379fs2bOvWGdzu4/P1/VvpjTTjbLNclP8T5f9R8s/5gAAABhWRG+ULS4u1vPPP6+qqiq5XC6VlpYqOzu71T+fTJ48WR6PR2VlZWpoaNCuXbu0bt06zZo1S5KUlJSkGTNm6Nlnn9WJEyd07tw5lZaW6pZbblFmZmbQsdasWaPx48fr2muvDdp+8uRJvfPOO3K5XPL5fPrss8/0+OOPa9KkSUpOTg7fDyJG1MtqmiUtm9ms0rAUf6g3wec2AABgEhGNdCUlJZo2bZpmz56toqIiVVdXq6ysTFarVTt27FBBQUFg5ZrU1FStWrVKW7duVWFhoRYsWKD58+dr2rRpgeM99NBDGjZsmKZPn66JEyfKZrPp6aefDjrnl19+qT//+c+tztK73W794he/0Fe/+lWNHj1a8+bN07hx4/Tkk0+G9wcRI8z08KmLjUqVTjdJh93RrgQAAKBzROxGWcnf2rJo0SItWrSoxb7CwkLt3LkzaFtubq5eeumlNo+XlJSkRx99VI8++mibY3r37t3mDQWZmZl65ZVXQqy+a/H6fKpXnLqZMNSPSJEs8s/WD0yKdjUAAABXz2TNF2hWd/5WhRQT/gbY46UhydJH9NUDAIAuwoSRDpLk8vhfzdh+I/lbcKrd0snQluYHAACIaYR6k6o1eajPP7/qKavgAACAroBQb1K159tvzNhTL0m9E6XMRFpwAABA10CoN6nATL2JfwPy7dJndZKzKdqVAAAAXB0TRzpzM3v7jeRvwfFJ2lUb7UoAAACuDqHepMx+o6wkDbRJPePpqwcAAMZHqDep5p56M4d6i8U/W7+nVmrwRrsaAACAjiPUm1Rz+003k/8GjEqVGn3+YA8AAGBUJo905kVPvd/13fwfbGjBAQAARkaoNylCvV+cRRqR6r9Z1uOLdjUAAAAdQ6g3KZdXssinZH4DlJ/q/5Czvy7alQAAAHQMkc6kaj1SkryyWCzRLiXqclOkeAsPogIAAMZFqDepWo+UZGHJF0lKskrDuvn76n204AAAAAMi1JuUyyMli1DfLD9VOtUoVbujXQkAAED7EepNyuVlpv5iI1Mli1gFBwAAGBOh3qRqmakPkhYvDU6mrx4AABgTod6k6KlvKT9VOuz2t+EAAAAYCaHepGo9UjKhPkh+qv+VFhwAAGA0hHqTqvX6l7TEBRmJUr9EQj0AADAeQr1JuTxSssUT7TJiTn6q9KlLcjZFuxIAAIDQEepNihtlWzfaLnkl/aM22pUAAACEjlBvUk5ulG3VtTbpmgTpI0e0KwEAAAgdod6EGrw+NfikFEJ9CxaLVJAq7XVJDlpwAACAQRDqTch5vpW+m+ipb82oVKnJJ205G+1KAAAAQkOoNyFHc6hnpr5Vg5OltDjpjVPRrgQAACA0hHoTCszUs/pNq6wWaZRdeues5PL4ol0OAADAFRHqTai5V5yZ+raNTpXqvNKfTke7EgAAgCsj1JuQg576K7q+m9QjXnr1y2hXAgAAcGWEehNy0lN/RXEW6Rs9pQ0nJbeXFhwAABDbCPUm5KCnPiTTe0k1Hm6YBQAAsY9Qb0KBnnqeKHtZX+ku9U6QXjwe7UoAAAAuj1BvQqx+E5p4izSzj7ThlORoogUHAADELkK9CTk8/gtvE0H1Smb1keq90vqT0a4EAACgbYR6E3J6JHu8ZLFEu5LYd3N3KStJWksLDgAAiGGEehNyeCR7XLSrMAarxaJ/7SNtPiN92cBfNgAAQGwi1JuQs0lKJdSHbHaG5PFJ61izHgAAxChCvQk5malvlxEpUm43WnAAAEDsItSbkMPDTH17WCwWzcqQtp2TPq+jBQcAAMQeQr0JOc7fKIvQ3dtXskj6zbFoVwIAANASod6EaL9pv2uTLJqcLv32C8njY7YeAADEFkK9CTmapBRCfbt9r5902C29dTralQAAAAQj1JsQS1p2zO29pPR46ddfRLsSAACAYIR6k/H4fKrzEuo7wma16Nt9pfVfSqcaacEBAACxI6Kh3uv1avny5SoqKlJBQYHmzJmj6urqNsfv2bNHxcXFys/P14QJE1ReXh60v76+XsuWLdO4ceM0evRoLVy4UGfPng3s/+CDD3TDDTeooKAg8PW1r30t6BhnzpzRwoULNXr0aI0bN07Lli1TQ0ND537jMcTp8b+y+k3HfK+f1OCTXmB5SwAAEEMiGupXrVqljRs3avXq1dq2bZv69++vuXPnyuv1thjrdDpVUlKi8ePHa/v27VqxYoVWrlypTZs2BcY8/vjjqqys1IYNG7Rlyxa5XC4tWbKkxbF27twZ+Nq6dWvQvh//+MdyuVzasmWLNmzYoMrKSj355JOd/83HiOZQz+o3HZOfatEYu/T/jko+bpgFAAAxIqKhfu3atSopKdHgwYOVkpKixYsXq6qqShUVFS3Gbt68WVarVQ888IBsNptGjRqlmTNnas2aNZL8s/Tr16/Xgw8+qIyMDHXv3l1LlizR22+/raNHj4ZUz5EjR7Rt2zYtWbJE3bt3V0ZGhh588EG9+uqrcrvdnfq9xwpHk/+Vmfora/JKB+t9Lb7u6CXtqpVeP3lh/xnacQAAQBRFLNQ7HA5VV1crLy8vsC0tLU1ZWVnau3dvi/H79u1Tbm6urNYLJebl5Wnfvn2SpAMHDsjtdmvEiBGB/UOGDFFycnKL402YMEFFRUX6t3/7N23fvj3oHMnJyRoyZEhg24gRI1RXV6eqqqqr/6ZjkKN5pp5Qf0W1HulPp1t+2eMkm0V66tCFbTWeaFcLAADMLGJNGE6nU5I/yF/MbrcH9l063m63B21LS0sLjG1+vXTMxccbPHiwfv/73+u6665TfX29XnnlFc2ZM0cvv/yyhg0b1uo5mv/fWk2XU1lZ2a7x0fL3plRJQ3V0/yfqF6/AX0ka0/vp4Beh3UuQl52ugwdDX9exPeONcuxca7o+rEnVje4j6mbx6pg7USdPR/7JVK39lQvGwfUzPq6hsXH9jI3rFyxioT41NVWSf8b+Yg6HI7Dv0vGnTp0K2lZTUxMYe/Hx0tPTWz1e79691bt378D473znO3r77bf1xhtvaNiwYUpNTW0R3pvra62my8nLy5PNZmvXe6Kh+qRP+j9pTO5Q6ZO/a8yYMZL8bSRZIZafmiplZdmvPLAD441y7Nvc0s4D0iH7tZpyjdQvXcoa1D/kY3eGioqKwPWD8XD9jI9raGxcP2Mz6/Vzu91tTiRHrP3GbrcrMzMzqBCHw6FDhw5p2LBhLcbn5ORoz549QTfR7t69Wzk5OZKk7Oxs2Wy2oOPt379fdXV1gTGtsVqtgRscc3Jy5HK5tH///sD+yspKJSUladCgQR3/ZmNYc0897TdXp79Nuj5Z2npO8tJODwAAoiyiN8oWFxfr+eefV1VVlVwul0pLS5Wdnd3qJ63JkyfL4/GorKxMDQ0N2rVrl9atW6dZs2ZJkpKSkjRjxgw9++yzOnHihM6dO6fS0lLdcsstyszMlCS9++67Onz4sLxer+rq6vS73/1OH374oSZPnixJGjBggMaPH6/S0lKdO3dOJ06c0LPPPqs777zTELPuHeGkp77TTOgpnWqUKmujXQkAADC7iIb6kpISTZs2TbNnz1ZRUZGqq6tVVlYmq9WqHTt2qKCgILByTWpqqlatWqWtW7eqsLBQCxYs0Pz58zVt2rTA8R566CENGzZM06dP18SJE2Wz2fT0008H9u/atUv33HOPRo8erYkTJ2rz5s36n//5n6Cba0tLS2Wz2TRx4kRNnz5dubm5Wrp0aeR+KBHmYJ36TjMqVeoeL71z9spjAQAAwimiq5VbrVYtWrRIixYtarGvsLBQO3fuDNqWm5url156qc3jJSUl6dFHH9Wjjz7a6v758+dr/vz5l60pPT1dP//5z0OovmtoDvUphPqrFmeRvtpd2nhK+rxOykqKdkUAAMCsIjpTj+hzevyz9FaLJdqldAlf6yHFW6RfhfZoBAAAgLAg1JuMo4nWm86UFi8VpUmvfikdc3PHLAAAiA5CvcnUerhJtrN9I11q8kkrjkS7EgAAYFaEepNxeJip72y9E6V/ukZ6rlo618RsPQAAiDxCvck4mKkPi7mZUo1H+mV1tCsBAABmRKg3GadHskd0zSNzGJ4ifaOn9PMjUr2H2XoAABBZhHqT4UbZ8FmaJX3RIK1kth4AAEQYod5k6KkPn4k9LZqWLj12UDrZwGw9AACIHEK9yTjpqQ+rp6/z/zXkZweiXQkAADATQr2J+Hy+wMOnEB7DUywq6S/98qj0iYvZegAAEBmEehOp9Ug+caNsuD0ySEq2Skv2R7sSAABgFoR6E3F6/K/M1IdXRqJFS7Ok35+UNp1ith4AAIQfod5EHOdDPT314ffDAVJeivS9fdKX3DQLAADCjFBvIk5CfcQkxVm0Olc63Sjd97H/fgYAAIBwIdSbiIP2m4gamWrRk0Ok109K/3M02tUAAICujFBvIo4m/ys3ykbOvw/wP2n2R59Je2qZrQcAAOFBqDcR2m/Cp8krHaz3tfg67JYeHSylxEnf3CXtcvp0ppFwDwAAOhdztiZC+0341Hqk98+2vf87faX/PizN2i29PlLqmRC52gAAQNfHTL2JsPpN9FzXTZqdIe11SU8cjHY1AACgqyHUmwjr1EfXV3pIE3tI/++Y9JtjtOAAAIDOQ/uNiTiapCSrFG+1RLsU0/pWH6nBJ839WLqhm083d+daAACAq8dMvYk4PLTeRFucRfrF9dK1SdKdldKRembsAQDA1SPUm0ith9abWNAzQVo/wn89/rlSqvMQ7AEAwNUh1JsIM/WxY3iKRS/kSn93SAs/i3Y1AADA6Aj1JuL08OCpWHJbL4sWXSv96qj0+klm6wEAQMcR8UzE0cT66LHm0cHSW2ekkn3SrrE+9bVd3Y2zZxp9qvG03H6yUXquWqpwSANsUlaSVGCXZvSSeiZwsy4AAEZHqDcRh0camBTtKnAxm9Wi1bk+Fe7wB/sNI32yWDoesms80p9OX/h/rUfafFrackZq9EmDk6W/1UgbT0k+SZ8OlO7PDO2vBGlxfAAAACBWEepNxMmNsjGhySsdvGjVm5Q4aclA6ZED0mMHpXv6Bofsjobpkw3SL45IJxqlQrs0vZfUN/F8DT7p18ekJw9Jh93SV3tc+XhT0vlLDwAAsYpQbyIOQn1MqPVI758N3tY3URqeIj16QPL4pH62C/s6EqaP1EvPHvGH9x9dK13fLXh/vEX6bj+pm1Vac1yyWaVxaR36dgAAQAzgRlmT8Pl83CgbwywW6d6+UqLVP4PedBX3zX7ikp457F8T/8cDWwb6ZvEWqXSINLSb9Jtj0vGGjp8TAABEF6HeJGo9/qDYnZn6mNU9Xro7Qzrkljae7NgxdtRIK49IPeOl/2+g1N92+fFJVmlOP/8HgIt78QEAgLEQ6k3iVJP/tVdidOvA5RXYpa909wfsj13te++OGp++u88f6H94begtO2nx/nO+f0463dj+mgEAQPQR6k3i5Pmwdg3tNzFvZh+pT4L0P9XSF+7Q3rPL6dPUf/gD/cJr/UG9PSan+183M1sPAIAhEepN4lRzqGf1kpiXZJV+MECyWvw3u16p1/3tMz5N2Cklx0kv5HZshZr0BOnm7tJfz0nnmjpWNwAAiB5CvUkQ6o2ld6I/2Nd6pO/slc61ceds+Rc+TfmH1D9RerdAuvYqnkMwJd1/38VbzNYDAGA4hHqTaA71vQj1hpGVJN2fKX1WJ+Vtl0oP+XSuyacmr09bzvj0WN1AfWev9NXu0rbRUnby1T0Yqk+ifz37rWclVytPpQUAALGLUG8SzaG+Jz31hpKbIv1umDQ0WVqyX8p6T+r7V+nWj6Q/NqZrbn/pj/lSj0560uuknpLbJ+10dsrhAABAhBDxTOJko9QjXoq3dk74Q+Tc1F361wyLKhw+rTwi+STd3kvqfXCXvnpDQaeeKzvJ/9ecihr/ijgAAMAYCPUmcbqRfnqjG2O36NfDLvy/4pC3089hsfhbcDaflhxNPKwMAACjoP3GJE41spwlQlNol7yiBQcAACMh1JvEqSZm6hGaTJuUkShVOKJdCQAACBWh3iRONbLyDULT3ILziYs16wEAMApCvUmcbPQ/YAgIxRi7/4bcvzNbDwCAIdBlbQINXp+cHtpvjKrJKx2sb/nwqcb0fi2213XS+vL9bf4HWu1wSBN7ds4xAQBA+BDqTYCnyRpbrUd6/2zL7Qe/aFCWLXjbTfbOO29hmvT6SelMo9ST3x0AAGJaRNtvvF6vLmA+ggAAGlxJREFUli9frqKiIhUUFGjOnDmqrq5uc/yePXtUXFys/Px8TZgwQeXl5UH76+vrtWzZMo0bN06jR4/WwoULdfbshfSzfv16FRcX///t3X90VNXd7/H3TMgvkgwBVCShJgUbEkwUAoiCFlEXivi4kKdY0EIfSZAf6pXK9VJjtUQrZZlaMXSVei+sriJaNWKh/JBFuxCoT0UfEIshSYEYAgQREkiYkGSSzOz7x2SGTBIkSDKTTD6vtWadmb139tln9hz4nnP22YfRo0czZswYMjIyKCoq8qlj5syZpKamMmLECO/rrbfe6tgNDzAF9fJdjIh2L788H9h2iIiIyKX5NahftWoVmzZtYu3atXz88cfExcUxb948XK7W821XV1eTmZnJbbfdxmeffcby5cv53e9+x9atW71lli5dSn5+Phs3buSjjz6ipqaGxYsXe/PPnz/Pk08+yc6dO9m1axcpKSlkZGRQW1vrs665c+eyb98+7+uRRx7pvC8hADxBvW6UlctxbZj7N/OlprYUERHp8vwa1L/zzjtkZmYyePBgoqKieOaZZygpKWHv3r2tym7btg2r1cqCBQsIDw9n+PDhTJs2jbfffhtwn6Vfv349Tz31FAMGDKBPnz4sXryYHTt2cOLECQAeeeQRxo0bR+/evQkPD2f+/PmUl5fz1Vdf+XOzA65cZ+rlO7BYIDUKimqgvuOfcyUiIiIdyG9j6u12O2VlZaSmpnrTbDYbCQkJFBYWMnr0aJ/yRUVFDBs2DKv1wnFHamoqeXl5ABw5cgSHw0FaWpo3f8iQIURGRlJYWEhcXFyrNnzyySdERkaSkJDgk7527VrWrFlD//79ufvuu5k/fz5RUVGXtX35+fmXVd6fPq/vDyRQVvgljdYGnzzPAVVDv4GUnqxvV32pif0oLT3T7vVfTvmeUHdHtqO0tLTT6ga4xhlBgxnAx0e+YVSDi/IzX7e7brm0tk5oSPeiPuze1H/dm/rPl9+C+upq9zV8m83mkx4TE+PNa1k+Jsb3rj+bzeYt61m2LHOx+oqLi3nuuedYvHgx0dHR3vSf/exnDB48GJvNxsGDB8nKyuL48eMsX778srYvNTWV8PDwSxcMgG2lBr6CCelpRIZYvOl79+5l5MiRgHt2lZY3XV5MdDQkJLT/jszLKd8T6u6odpSWlrY6QO3obYxzwfrD8E3UAAYOhITvtz5Ylu+m+f4n3ZP6sHtT/3VvPbX/HA7HRU8k+234jSeQttt9J7622+0+QXbz8i2D83PnznnLXk59Bw8e5Kc//SmZmZnMmDHDJy89PZ3Y2FisVivJyclkZWWxbds26urqvsNWdk0VDRBpxSegF2mPUCsk94b882Baz6opIiIiXYTfgvqYmBji4+N9ji7sdjtHjx4lJSWlVfnk5GQKCgp8bqI9cOAAycnJACQmJhIeHu5TX3FxMbW1td4yAF9++SWzZs1i7ty5zJkz55Lt9Az3MUEUwZzR02TlCqRGuw8MD9deuqyIiIgEhl9vlJ0+fTqrV6+mpKSEmpoacnJySExMbPPyycSJE3E6naxcuZL6+nr2799PXl6e90x7REQEU6ZMITc3l1OnTlFVVUVOTg7jx48nPj4egD179jB79mwWLVrEzJkzW62jvLycnTt3UlNTgzGGw4cPs3TpUu68804iIyM798vwo/IG3SQr311q0+0l288Gth0iIiJycX4N6jMzM5k0aRIPP/wwY8eOpaysjJUrV2K1WtmzZw8jRozwzlwTHR3NqlWr2LVrF6NGjeLJJ5/k8ccfZ9KkSd76srKySElJYfLkyUyYMIHw8HBeeeUVb/7rr7+O3W5n6dKlPvPQ//WvfwXc45JWrFjB7bffTnp6OvPnz+fmm29m2bJl/vxaOl2Fgnq5Av1CIT4cPmrjAVgiIiLSNfj1ibJWq5VFixaxaNGiVnmjRo1i3759PmnDhg3j3XffvWh9ERERvPTSS7z00ktt5r/55pvf2p74+Hjef//9drS8e6togO9FBLoV0p2lRcHfzkBlgyE2VPdmiIiIdDV+PVMvgVHRCP38evgmwSY1CpzANg3BERER6ZIU1Ac5pzGc1fAbuUKDIyG2F2ypCHRLREREpC0K6oNcZSO40Ow3cmWsFhgfCx9WgCuIZoYSEREJFgrqg1xF0wNkdaZertSEWDjdAP9zLtAtERERkZYU1Ac5BfXSUcbHuv/B2KwhOCIiIl2Ogvogp6BeOkpsKNzaR+PqRUREuiIF9UFOQb10pPv6w+fV8LVD4+pFRES6EgX1Qc4T1OtGWekIk/u7lzpbLyIi0rUoqA9y5Q3QywK2kEC3RIJBWhQMCldQLyIi0tUoqA9yngdPWSx6CqhcOYvFwn394W9nweHSEBwREZGuQkF9kDujB09JB5vcH6qdsKsy0C0RERERDwX1Qa5CQb10sLv6Qm8r/OV0oFsiIiIiHgrqg1xFg26SlY7VO8Q9BGd9OTj1dFkREZEuQUF9kCtvgH4K6qWDTb0aTtbDP6sC3RIREREBBfVBzRij4TfSKSb3h3ArrNMQHBERkS5BQX0Qq3ZCvYH+vQLdEgk2Mb0sTOzrHldvNARHREQk4BTUB7GSOvcyISKw7ZDg9J/XwDEH/I890C0RERERBfVB7FCNe5nUO7DtkOD0H/3dDzbTEBwREZHAU1AfxA7WupfXRwa2HRKc+oZauKsvfKAhOCIiIgGnoD6IHaqBgWHu8c8inWHq1VBcC/vPB7olIiIiPZuC+iB2qBZ+oLP00ommXOUegvPmyUC3REREpGdTUB/EDtXADzSeXjrR1WEWHrgK1pwEh0tDcERERAJFQX2Qqmo0nGrQmXrpfJkD3Q8521Ae6JaIiIj0XJrBPEhp5hvpaI0uKK1rfTY+qTfEhcGK4zDGdiHfFuK+mVZEREQ6n4L6IOWZ+UbDb6SjnHfC7sq280bGwMYKWHsSrg5zp93TD/rqacYiIiJ+oeE3QepQDViAIXrwlPjB2D7u39t/VwW6JSIiIj2TgvogdagWrouAiBANf5DO1zcUUqPgkypw6n5ZERERv1NQH6QO1UCSbpIVPxrXB6qc8GV1oFsiIiLS8yioD0LGGA7WwvUaTy9+lBYN/UNhSwXoAbMiIiL+paA+CJU3QFWjztSLf4VYYHJ/OOqAL3S2XkRExK8U1Aehg03TWWrmG/G3MTYYEAYbyzW2XkRExJ8U1AehQ03TWepMvfhbiAX+oz+cqIdNehiViIiI3yioD0IHa6CXBRI1naUEQHoMxIfD8uPQ4NLpehEREX9QUB+EDtXC4AjoZdV0luJ/Vgs8cBUcqYP/eyLQrREREekZFNQHoUM1Gk8vgXVjlHuKy/9TDPnVOlsvIiLS2RTUBxmXMRyqhR9oPL0EkMUCr10PMSEw/QCc112zIiIinUpBfZD5qhZqXZCkM/USYFeHwdphUFgD/+tQoFsjIiIS3BTUB5l3TrmXk/oHth0iAHf3s/BsAvzxa1hZprP1IiIinUVBfRAxxvDmSbgjFhIidJOsdA1LEmFSP3j8IGQVG1x63KyIiEiHU1AfRHafc898M+vaQLdE5IJeVgvr02BOHCw76h5jX6Mx9iIiIh1KQX0QWXMSIq3wn1cHuiUivkKtFv6QBL8ZAutOww2fwf87YajXPPYiIiIdQkF9kKhzGt49BVOvhpheGnojXY/FYuHp6yz8bTgMCIW5/4ahn0LOUcOhGgX3IiIiV6JXoBsgHWNTBVQ2wkwNvZEuotEFpXWtg/UhkfDODbCzElYch8XF7ldKb8PEfu757cf2gbhwHZyKiIi0l4L6IPHmSYgLg7v6BrolIm7nnbC78tvLPBYP5Q3Q4IKPq+CNE/D6cXdefLghLQpuiIKhvSEu3P0b7x8KoRYItTYtm14hFvfVABERkZ5IQX0Q+Fe14cMz8LPvQYiCGulmrgqFu2LhvwZCvcs9r/1eO+RXw79rYPtZqG/n6JxQi6GXxX1vybVh7gOBxAj3E5avC4fvRUB8uDsv3Kp9RUREgodfg3qXy8Xy5ct5//33qa2tJT09nRdffJH4+Pg2yxcUFPDiiy9SWFhI3759mT17NrNmzfLm19XVsXTpUrZu3UpjYyM//OEPWbJkCbGxsd4ymzZtIjc3l5MnT5KYmMizzz7Lrbfe6s0/duwYS5Ys4fPPPycyMpJp06axcOHCbnPGb3O5YUYBXBMKC9r+GkW6vJZn9ePD3a97+oPTwNlGONfoHmJ23ukO0A/XufNcxr100rQ04HC5yxaeh0/Pgd3Zep39Qw0Dw3C/wqFPL7CFgK1pGfMtn3VAICIiXY1fg/pVq1axadMm1q5dy4ABA1i2bBnz5s1jw4YNWK2+9+xWV1eTmZnJww8/zJ/+9CcKCwt57LHHuOaaa7j33nsBWLp0Kfn5+WzcuJGIiAieeeYZFi9ezBtvvAHA559/TlZWFq+//jrjxo1jw4YNzJ8/ny1bthAXF4fT6WTevHmkp6eTm5vLN998Q2ZmJjabjYyMDH9+Nd/J8mOG/30YhkfDhhshXmOQJQiFWNxn868KvZB2Swzstrfv7++KhRoXfF0PJxxwqh6+aYDT9RfeHzjvDvyrndCeiwJhFuMT5MeENHvf7HNUCESGQITVffXAs+xlgaLGGE6WG+/BSGPTAUm9cQ9HqjfQYNxXL5zmwpCjsOZLC4RdJK3lsq08KxqyJCISLPwa1L/zzjtkZmYyePBgAJ555hnGjh3L3r17GT16tE/Zbdu2YbVaWbBgAVarleHDhzNt2jTefvtt7r33Xurq6li/fj0rVqxgwIABACxevJj77ruPEydOEBcXx3vvvcedd97JhAkTAJg2bRrvvfceH3zwAU888QR79uyhtLSUP//5z0RFRTF48GAyMzNZvXp1u4N60/Qgnfr6+o76mtrlZL0hpxh+epV7msDeWHA4Lr8eR9MfOesNYY3t+xvTQLvLXm75nlB3R7WjN07CGh3tKtsRbemudVfXwb7qC5+vscA1YUBY67JjY6AeqHW5A/waF9Q43a9qF9Q0XkirdrrLnW8qd94B5TVwvim/znWpll0H+d9hp+0EFs/L4l5aLRfSPBclrM3zm733/J21eT341uOpwyet6e8AjAEX7oMXg/vqiwv3wU+IxXfZywIhXHjfMs/zOaStfJrqb1qXC98rPa5mV348ZTzpnos9zddvr4unf0Ed1rbaiu/nkKbtvphLHUxeSf6VPu+tPev29Jlp9v25mvKcbeU162twfz/Wpu/JyoXvzIr7+wZowH2Q29h0wOs5+K1zua/O1brc7+tc7rzmPN+953cbYoEGxxB6f1bns+4Q2lh/U77Vk9+0tDRtn+f7af7e+9n45nnLmgvvrZ4D7qZlr2YH6qG4l72a/X5a/o48+1HLfMtF8pu3z/veXCT9Iu9b/s1F672CdVyq/tOOq9l6qM6nvBP378tzkqT5q9Fc+L15+tOnf2n9O/SmtVEupTeM7+v/kyKeeNO08QX5Lai32+2UlZWRmprqTbPZbCQkJFBYWNgqqC8qKmLYsGE+Z/BTU1PJy8sD4MiRIzgcDtLS0rz5Q4YMITIyksLCQuLi4igqKuL+++/3qTc1NZWioiLvOhISErDZbD75x48fp7q6mujo6EtuV0NDAwAHDx5s71fRYf4aA9TDV4XfvY78/Hzv+7RvKddc4+n2l73c8j2h7o5qR1oYcLqyXWU7oi09oW77affSCtiaXu0WwoXoQzpfywgqECKA2rIAN0J8eCKx9vi2CKgr/L7k24UD504Hbv1nIT+Au39DQwMRERE+aX4L6qur3afKmgfQADExMd68luVjYmJ80mw2m7esZ9myTPP6qqurW63PZrNRUlLyrevw5LUnqI+KiiIpKYnQ0FBdxhYRERGRTmOMoaGhgaioqFZ5fgvqPQGy3e47ENZut7cZPEdHR1NRUeGTdu7cOW/Z5vX169evzfqio6Nbra9lHS0PKM6dO+dT/6VYrdZWBwYiIiIiIp2h5Rl6D789UTYmJob4+Hif4R52u52jR4+SkpLSqnxycjIFBQW4XBcGxh04cIDk5GQAEhMTCQ8P96mvuLiY2tpab5nk5GSf/JZ1JCcnU1pa6hP4HzhwgEGDBrU7qBcRERERCTS/BfUA06dPZ/Xq1ZSUlFBTU0NOTg6JiYmMHDmyVdmJEyfidDpZuXIl9fX17N+/n7y8PGbMmAG4j1KmTJlCbm4up06doqqqipycHMaPH++dIvOhhx5i+/bt7Ny5k4aGBtatW8fBgwd58MEHARg1ahTXXXcdOTk51NTUUFJSwqpVq7zrEBERERHpDiymrdtnO4nL5eK1117zzlM/cuRIsrOzGTRoEHv27GHOnDls3ryZuLg4wD1PfXZ2tnee+oyMjFbz1L/88sts3boVp9PJ7bffTnZ29kXnqU9ISCArK6vNeer37t1LZGQkDz30ULeap15ERERExK9BvYiIiIiIdDy/Dr8REREREZGOp6BeRERERKSbU1AvIiIiItLNKagXEREREenmFNSLiIiIiHRzCup7GJfLxW9/+1vGjh3LiBEjyMjIoKysLNDNkiabN2/m4YcfJj09naFDh7bKLygoYPr06dx0003ccccdrFmzxie/rq6OF154gZtvvpn09HQWLlxIZWWlv5rf4+Xk5DB58mTS09O57bbbyMrK4uzZsz5l1Idd1+9//3vuvvtuRo4cyZgxY8jIyKCwsNCbr77rXh5//HGGDh3Kp59+6k375z//yQMPPMBNN93EPffcw5YtW3z+5uzZsyxcuJD09HRuvvlmXnjhBerr6/3d9B5rxYoVpKSkMGLECO/r6aef9uZrH7wEIz3KG2+8YSZMmGCKi4tNdXW1+cUvfmHuv/9+43Q6A900Mcbs2rXLbNy40eTl5ZmkpCSfPLvdbm699VazYsUKU1dXZ/bt22dGjx5tPvzwQ2+Z559/3jz44IPm5MmTprKy0syZM8c89thj/t6MHuvVV181Bw4cMPX19aa8vNw8+uijZu7cud589WHX9tVXX5nKykpjjDEOh8OsXr3ajBs3zjidTvVdN/OXv/zFzJ492yQlJZndu3cbY4w5duyYufHGG817771nHA6H2b59u7nxxhvNF1984f272bNnmzlz5pjKykpz8uRJ8+CDD5rs7OxAbUaPk5uba37yk5+0mad98NIU1PcwEyZMMG+99Zb3c1VVlbnhhhvMZ599FsBWSUu7d+9uFdSvW7fOG2B4vPLKK2bmzJnGGGNqa2tNWlqa2bFjhzf/8OHDJikpyZSVlfmn4eJj+/btZsSIEd7P6sPuw+FwmD/+8Y8mKSnJVFZWqu+6ka+//tqMHz/elJWV+QT1ubm55kc/+pFP2aeeesr8/Oc/N8a4g/6kpCRz+PBhb/6OHTvMTTfdZOrq6vy3AT3YtwX12gcvTcNvehC73U5ZWRmpqaneNJvNRkJCgs8lZumaioqKGDZsGFbrhd02NTWVoqIiAI4cOYLD4SAtLc2bP2TIECIjI9W/AfLJJ5+QnJzs/aw+7Pp27NjBqFGjSEtLY9myZTz66KP06dNHfddNGGPIyspi/vz53qfTexQVFfn8/we+fVhUVERkZCRDhgzx5qelpVFbW0tJSUnnN14AyM/P55ZbbmHChAksWrSIY8eOAfr3sz16BboB4j/V1dWAO5BvLiYmxpsnXVd1dTUxMTE+aTabzdt3nmXLMurfwNiyZQt5eXmsXbvWm6Y+7PruuOMO9uzZQ2VlJevXr2fgwIGA+q67ePvttzHG8OMf/7hVXnV1Nddff71PWss+bKv/PHnS+e655x6mTp1KXFwcp06d4tVXX+XRRx9lw4YN2gfbQUF9DxIdHQ24z9g3Z7fbvXnSdUVHR1NRUeGTdu7cOW/fNe/ffv36ecuof/1v8+bNLFmyhJUrV3LDDTd409WH3UdsbCyzZs1i9OjRDB48WH3XDRw9epSVK1fy7rvvtpkfHR3d6v+/ln3YMvjzlFcf+kdSUpL3/YABA3j55ZcZNWoU+/bt0z7YDhp+04PExMQQHx9Pfn6+N81ut3P06FFSUlIC2DJpj+TkZAoKCnC5XN60AwcOeId3JCYmEh4e7tO/xcXF1NbW+gwBkc6Vl5dHdnY2f/jDH7jlllt88tSH3YvL5aKxsZHS0lL1XTfgucIydepUxowZw5gxYwBYsGABv/zlL0lOTvbpH/Dtw+TkZGpqaiguLvbm5+fnExERwfe//33/bYh4WSwWLBYLxhjtg+2goL6HmT59OqtXr6akpISamhpycnJITExk5MiRgW6aAE6nE4fDQUNDAwAOhwOHw4HL5WLixIk4nU5WrlxJfX09+/fvJy8vjxkzZgAQERHBlClTyM3N5dSpU1RVVZGTk8P48eOJj48P5Gb1GGvWrOE3v/kNq1evbnOfUh92bWvWrOH06dMAnDlzhuzsbMLCwhg+fLj6rhuYNGkSf//739mwYYP3BfCrX/2Kp59+milTpvDvf/+bdevW0dDQwM6dO/noo4+YPn06AIMGDeK2224jJyeHqqoqTp06RW5uLlOnTiU8PDyQm9ZjbNmyhTNnzgBQUVHB888/T79+/RgxYoT2wXawGGNMoBsh/uNyuXjttdd4//33qa2tZeTIkWRnZzNo0KBAN02ADz74gGeffbZV+po1axgzZgwFBQVkZ2dTWFhI3759ycjIYNasWd5ydXV1vPzyy2zduhWn08ntt99OdnY2sbGx/tyMHmvo0KH06tWLsLAwn/TNmzd7b9pTH3ZdCxYs4F//+hfnz58nOjqatLQ0nnjiCe8QKvVd9zN06FDvv5/gnqf+17/+NaWlpVx77bUsXLiQ++67z1veczD3j3/8g5CQECZNmsRzzz2noN5P5s2bxxdffEFtbS02m43Ro0fz1FNPkZCQAGgfvBQF9SIiIiIi3ZyG34iIiIiIdHMK6kVEREREujkF9SIiIiIi3ZyCehERERGRbk5BvYiIiIhIN6egXkRERESkm1NQLyIiIiLSzSmoFxERERHp5v4/qnM/aZEy9FgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpP0hsuJRxzg"
      },
      "source": [
        "#Dataset Creation\n",
        "class GoogleReviews(data.Dataset):\n",
        "  def __init__(self,reviews,targets,tokenizer,max_len):\n",
        "      self.reviews=reviews\n",
        "      self.targets=targets\n",
        "      self.tokenizer=tokenizer\n",
        "      self.max_len=max_len\n",
        "  def __len__(self):\n",
        "      return (len(self.reviews))\n",
        "  def __getitem__(self,item):\n",
        "      review=str(self.reviews[item])\n",
        "      target=self.targets[item]\n",
        "      encoding=tokenizer.encode_plus(\n",
        "          review,\n",
        "          max_length=self.max_len,\n",
        "          add_special_tokens=True,\n",
        "          pad_to_max_length=True,\n",
        "          return_attention_mask=True,\n",
        "          return_token_type_ids=False,\n",
        "          return_tensors='pt' #return pytorch tensors pt means pytorch tensors\n",
        "      )\n",
        "\n",
        "      #now we get the encoding dict in which we have token_ids,attn_ids and all\n",
        "      return {\n",
        "          'review_text':review,\n",
        "          'input_ids':encoding['input_ids'].flatten(),\n",
        "          'attention_mask':encoding['attention_mask'].flatten(),\n",
        "          'targets':torch.tensor(target,dtype=torch.long)\n",
        "      }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIlw8CmqRx1N"
      },
      "source": [
        "MAX_LEN=160\n",
        "BATCH_SIZE=16\n",
        "EPOCHS=20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dz6Edp3mRx5O"
      },
      "source": [
        "df_train,df_test=train_test_split(df,test_size=0.2,random_state=RANDOM_SEED)\n",
        "df_val,df_test=train_test_split(df_test,test_size=0.5,random_state=RANDOM_SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1RlUi33Rx61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0051e733-1d28-40a7-950c-c52f9da5f243"
      },
      "source": [
        "print(df_train.shape,df_val.shape,df_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(12596, 12) (1575, 12) (1575, 12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCbanKPGRx9_"
      },
      "source": [
        "def create_data_loader(df,tokenizer,max_len,batch_size):\n",
        "  ds=GoogleReviews(\n",
        "      reviews=df.content.to_numpy(),\n",
        "      targets=df.sentiment.to_numpy(),\n",
        "      tokenizer=tokenizer,\n",
        "      max_len=max_len\n",
        "  )\n",
        "  return data.DataLoader(\n",
        "      ds,\n",
        "      batch_size=batch_size,\n",
        "      num_workers=4\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0voE90p7Rx_x"
      },
      "source": [
        "train_data_loader=create_data_loader(df_train,tokenizer,MAX_LEN,BATCH_SIZE)\n",
        "val_data_loader=create_data_loader(df_val,tokenizer,MAX_LEN,BATCH_SIZE)\n",
        "test_data_loader=create_data_loader(df_test,tokenizer,MAX_LEN,BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxQfjfnVRyDQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5785034f-c791-4191-97a3-91dbba775ac0"
      },
      "source": [
        "data_checking=next(iter(train_data_loader))\n",
        "data_checking.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['review_text', 'input_ids', 'attention_mask', 'targets'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKD9m9o8RyFO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49b224ce-d5b9-4905-863e-6baf2b1be48b"
      },
      "source": [
        "print(data_checking['input_ids'].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([16, 160])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYCOIjaxRyNQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f648f8c6-24c3-4f1d-dc86-f51876b0632f"
      },
      "source": [
        "print(data_checking['attention_mask'].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([16, 160])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_4Q-JRyRyO4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b37b2b75-1570-4a65-95dc-63b4ff52e766"
      },
      "source": [
        "print(data_checking['targets'].shape)\n",
        "print(data_checking['targets'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([16])\n",
            "tensor([1, 1, 1, 0, 1, 2, 2, 0, 0, 0, 2, 2, 0, 1, 1, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5KYFxfVAurW"
      },
      "source": [
        "import torch.nn\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rGlxRUFas17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c5fcce4-d9d6-408a-8f03-faab321e18da"
      },
      "source": [
        "print(len(class_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhoEcLKtQMkC"
      },
      "source": [
        "#Building Sentiment Classifier\n",
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self,num_class):\n",
        "      super().__init__()\n",
        "      self.num_class=num_class\n",
        "      self.bert=BertModel.from_pretrained('bert-base-cased')\n",
        "      self.drop=nn.Dropout(p=0.3)\n",
        "      self.out=nn.Linear(self.bert.config.hidden_size,num_class)\n",
        "      \n",
        "    def forward(self,input_ids,attention_mask):\n",
        "      #print(input_ids.shape,attention_mask.shape)\n",
        "      #print(input_ids,attention_mask)\n",
        "      _,pooled_output=self.bert(input_ids=input_ids,attention_mask=attention_mask)\n",
        "      #print('number of classes',self.num_class)\n",
        "      #print('pooled out shape',pooled_output.shape)\n",
        "      output=self.drop(pooled_output)\n",
        "      #print('from model')\n",
        "      output=self.out(output)\n",
        "      #print('output shapes',output.shape)\n",
        "    \n",
        "      return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8MH3eb09Ap1"
      },
      "source": [
        "#model_checking=SentimentClassifier(len(class_names)).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Upsadkc2Tsod",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d8eae64-100f-488e-ded0-20e8c86ac6c4"
      },
      "source": [
        "for dic in train_data_loader:\n",
        "  print(dic['input_ids'].shape,dic['attention_mask'].shape,dic['targets'].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([16, 160]) torch.Size([16, 160]) torch.Size([16])\n",
            "torch.Size([4, 160]) torch.Size([4, 160]) torch.Size([4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZHxZaMGOruZ"
      },
      "source": [
        "#checking that our model is working or not\n",
        "#input_ids_checking=data_checking['input_ids'].to(device)\n",
        "#attention_mask_checking=data_checking['attention_mask'].to(device)\n",
        "#print(input_ids_checking.shape)\n",
        "#print(attention_mask_checking.shape) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtbxPvEmPWim"
      },
      "source": [
        "#model_checking(input_ids_checking,attention_mask_checking) #so here we get 3 output for the batch of 16 #3 classes neutral,negative,positive "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXlIk5NTPcT0"
      },
      "source": [
        "#Training \n",
        "#creatig optimizer\n",
        "\n",
        "#creating main model for training\n",
        "model=SentimentClassifier(len(class_names)).to(device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9I7d2MdaKUlB"
      },
      "source": [
        "from transformers import  AdamW\n",
        "optimizer=AdamW(model.parameters(),lr=2e-5,correct_bias=False)\n",
        "\n",
        "#creating scheduler\n",
        "total_steps=len(train_data_loader) * EPOCHS\n",
        "scheduler=get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "#creating loss function\n",
        "loss_fn=nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfXRSudfaOyY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf1b1156-ef85-4d84-afb8-f24736c7edf3"
      },
      "source": [
        "i=torch.tensor([[0.1,0.1,0.8]])\n",
        "o=torch.tensor([2])\n",
        "print(i,o)\n",
        "loss_fn(i,o)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.1000, 0.1000, 0.8000]]) tensor([2])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.6897)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHx2jdTHUG-t"
      },
      "source": [
        "def train_epoch(model,\n",
        "    data_loader,\n",
        "    loss_fn,\n",
        "    optimizer,\n",
        "    device,\n",
        "    scheduler,\n",
        "    n_examples):\n",
        "  \n",
        "  #print(n_examples)\n",
        "  #print(device)\n",
        "  model.train()\n",
        "  \n",
        "  #print(id(model))\n",
        "\n",
        "  losses=[]\n",
        "  correct_predictions=0\n",
        "\n",
        "  for d in data_loader:\n",
        "    #print(d['input_ids'],d['attention_mask'],d['targets'])\n",
        "    \n",
        "    #print(d['input_ids'].shape,d['attention_mask'].shape,d['targets'].shape)\n",
        "    input_ids=d['input_ids'].to(device)\n",
        "\n",
        "    attention_mask=d['attention_mask'].to(device)\n",
        "    targets=d['targets'].to(device)\n",
        "\n",
        "    #print(input_ids,attention_mask,targets)\n",
        "    #print(input_ids.shape,attention_mask.shape,targets.shape)\n",
        "\n",
        "    outputs=model(input_ids=input_ids,attention_mask=attention_mask)\n",
        "    #print('output shape',outputs.shape)\n",
        "\n",
        "    _,preds=torch.max(outputs,dim=1) #this function return tuple of (values,index_of_max_value) in this case we are using index\n",
        "    #print(outputs,targets)\n",
        "    #print(preds.shape,targets.shape)\n",
        "    loss=loss_fn(outputs,targets)\n",
        "    #print('the loss is',loss)\n",
        "    correct_predictions+=torch.sum(preds==targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    #doing some gradient clipping to avoid the exploding gradient issue\n",
        "    nn.utils.clip_grad_norm_(model.parameters(),max_norm=1.0)\n",
        "\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "  return correct_predictions.double()/n_examples,np.mean(losses)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP-UUOHfdwkl"
      },
      "source": [
        "def eval_model(model,data_loader,loss_fn,device,n_examples):\n",
        "  \n",
        "  model.eval()\n",
        "  print(id(model))\n",
        "\n",
        "  losses=[]\n",
        "  correct_predictions=0\n",
        "  with torch.no_grad():\n",
        "      for d in data_loader:\n",
        "        input_ids=d['input_ids'].to(device)\n",
        "        attention_mask=d['attention_mask'].to(device)\n",
        "        targets=d['targets'].to(device)\n",
        "\n",
        "        outputs=model(input_ids=input_ids,attention_mask=attention_mask)\n",
        "\n",
        "        _,preds=torch.max(outputs,dim=1) #this function return tuple of (values,index_of_max_value) in this case we are using index\n",
        "        \n",
        "        loss=loss_fn(outputs,targets)\n",
        "        correct_predictions+=torch.sum(preds==targets)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double()/n_examples,np.mean(losses)      \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E3ANUcjAu_U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7c2fa27-66c7-489a-b41c-aa6a2c0e59dd"
      },
      "source": [
        "len(df_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12596"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHzz6nM2K2dv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "074cb083-82bf-4ea3-85fc-73bc0c4cf996"
      },
      "source": [
        "print(id(model))\n",
        "print(loss_fn)\n",
        "print(optimizer)\n",
        "print(scheduler)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "140652116545888\n",
            "CrossEntropyLoss()\n",
            "AdamW (\n",
            "Parameter Group 0\n",
            "    betas: (0.9, 0.999)\n",
            "    correct_bias: False\n",
            "    eps: 1e-06\n",
            "    initial_lr: 2e-05\n",
            "    lr: 2e-05\n",
            "    weight_decay: 0.0\n",
            ")\n",
            "<torch.optim.lr_scheduler.LambdaLR object at 0x7fec19f16ef0>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyLZnsAOkKqL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87e53716-0b20-48a4-ea4f-42e4f2987815"
      },
      "source": [
        "#%%time\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,    \n",
        "    loss_fn, \n",
        "    optimizer, \n",
        "    device, \n",
        "    scheduler, \n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn, \n",
        "    device, \n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  if val_acc > best_accuracy:\n",
        "    torch.save(model.state_dict(), '/content/drive/My Drive/Colab Notebooks/SentimentClassifier/SentimentClassifier.pth')\n",
        "    best_accuracy = val_acc\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.7516179159472739 accuracy 0.6510797078437599\n",
            "140652116545888\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.6082443218640606 accuracy 0.7466666666666667\n",
            "\n",
            "Epoch 2/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.44519474297326983 accuracy 0.8289933312162592\n",
            "140652116545888\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.5935942178422754 accuracy 0.7930158730158731\n",
            "\n",
            "Epoch 3/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2637555825708406 accuracy 0.9129088599555415\n",
            "140652116545888\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.6336027378536234 accuracy 0.8393650793650793\n",
            "\n",
            "Epoch 4/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.17834010713708323 accuracy 0.9468879009209273\n",
            "140652116545888\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.7334421064135515 accuracy 0.8476190476190476\n",
            "\n",
            "Epoch 5/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.13032419915754392 accuracy 0.963639250555732\n",
            "140652116545888\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.7999540021774745 accuracy 0.8482539682539683\n",
            "\n",
            "Epoch 6/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.10584504057036898 accuracy 0.9722134010797079\n",
            "140652116545888\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.9021552279168232 accuracy 0.8533333333333333\n",
            "\n",
            "Epoch 7/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.08079627305658524 accuracy 0.9780882819942839\n",
            "140652116545888\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 1.0437085791955458 accuracy 0.8457142857142858\n",
            "\n",
            "Epoch 8/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.07469266689204325 accuracy 0.9798348682121308\n",
            "140652116545888\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.9992307590422422 accuracy 0.8533333333333333\n",
            "\n",
            "Epoch 9/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.060579298691507036 accuracy 0.9828516989520483\n",
            "140652116545888\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 1.0189160226069325 accuracy 0.8609523809523809\n",
            "\n",
            "Epoch 10/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0555274219920006 accuracy 0.9846776754525246\n",
            "140652116545888\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 1.0984238709042065 accuracy 0.8571428571428571\n",
            "\n",
            "Epoch 11/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.041306330548054736 accuracy 0.9880120673229597\n",
            "140652116545888\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 1.189755562661131 accuracy 0.8501587301587301\n",
            "\n",
            "Epoch 12/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.04509105263943712 accuracy 0.9870593839314068\n",
            "140652116545888\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 1.238019957915116 accuracy 0.8438095238095238\n",
            "\n",
            "Epoch 13/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.03475445593121641 accuracy 0.9880120673229597\n",
            "140652116545888\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 1.2526119636857385 accuracy 0.8488888888888889\n",
            "\n",
            "Epoch 14/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.03450255140064586 accuracy 0.9880120673229597\n",
            "140652116545888\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 1.188150430349013 accuracy 0.8584126984126984\n",
            "\n",
            "Epoch 15/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.030378943800328878 accuracy 0.989441092410289\n",
            "140652116545888\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 1.2680445346664746 accuracy 0.8507936507936508\n",
            "\n",
            "Epoch 16/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.02803585197336804 accuracy 0.9900762146713242\n",
            "140652116545888\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 1.2327870257130682 accuracy 0.8584126984126984\n",
            "\n",
            "Epoch 17/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.02659706352001125 accuracy 0.9896792632581772\n",
            "140652116545888\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 1.239323860038205 accuracy 0.8558730158730159\n",
            "\n",
            "Epoch 18/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.023374656766556222 accuracy 0.9912670689107653\n",
            "140652116545888\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 1.2539302927846556 accuracy 0.8571428571428571\n",
            "\n",
            "Epoch 19/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.020904029777759972 accuracy 0.9911082883455066\n",
            "140652116545888\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 1.2711940026562265 accuracy 0.8590476190476191\n",
            "\n",
            "Epoch 20/20\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.020247684636633772 accuracy 0.9912670689107653\n",
            "140652116545888\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 1.2595787729606496 accuracy 0.8609523809523809\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Zefv_FMWdID",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "outputId": "eb1c6ba8-8ec1-4707-d5c0-92b0d2e31b70"
      },
      "source": [
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "At1xyFkVgbn-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcdef06b-cd5c-41af-b4e7-9cb033aadff4"
      },
      "source": [
        "#Evaluation\n",
        "test_acc, _ = eval_model(\n",
        "  model,\n",
        "  test_data_loader,\n",
        "  loss_fn,\n",
        "  device,\n",
        "  len(df_test)\n",
        ")\n",
        "\n",
        "test_acc.item()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "140652116545888\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8596825396825397"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAQj3ipPgb1U"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  \n",
        "  review_texts = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      texts = d[\"review_text\"]\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "      review_texts.extend(texts)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(probs)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return review_texts, predictions, prediction_probs, real_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwrFPyAVgb3i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e6d80de-3be5-444f-b040-d4e3cb73e75d"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")\n",
        "print(classification_report(y_test, y_pred, target_names=class_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.91      0.83      0.87       519\n",
            "     neutral       0.77      0.85      0.81       466\n",
            "    positive       0.90      0.89      0.90       590\n",
            "\n",
            "    accuracy                           0.86      1575\n",
            "   macro avg       0.86      0.86      0.86      1575\n",
            "weighted avg       0.86      0.86      0.86      1575\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p302X9Rqgb8s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "outputId": "72db59ca-f346-4d0b-c8a4-991bc5ab4236"
      },
      "source": [
        "def show_confusion_matrix(confusion_matrix):\n",
        "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
        "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
        "  plt.ylabel('True sentiment')\n",
        "  plt.xlabel('Predicted sentiment');\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "show_confusion_matrix(df_cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVpJB3zsgcB4"
      },
      "source": [
        "\n",
        "idx = 2\n",
        "\n",
        "review_text = y_review_texts[idx]\n",
        "true_sentiment = y_test[idx]\n",
        "pred_df = pd.DataFrame({\n",
        "  'class_names': class_names,\n",
        "  'values': y_pred_probs[idx]\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzm3oeYSgb7s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc94d55e-1172-454e-924c-038c503eee63"
      },
      "source": [
        "from textwrap import wrap\n",
        "print(\"\\n\".join(wrap(review_text)))\n",
        "print()\n",
        "print(f'True sentiment: {class_names[true_sentiment]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "it doesnt do any thing its gliched everytime i open it\n",
            "\n",
            "True sentiment: negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3_3P-bZiAzi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "outputId": "df28f58d-c5da-4fae-8960-697d29bd171d"
      },
      "source": [
        "sns.barplot(x='values', y='class_names', data=pred_df, orient='h')\n",
        "plt.ylabel('sentiment')\n",
        "plt.xlabel('probability')\n",
        "plt.xlim([0, 1]);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm0zH0e6iHWR"
      },
      "source": [
        "review_text = \"I love completing my Instagram! Best app ever!!!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kaoGKrIiNiI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "451e8f43-d3d8-48a3-b457-3c7483d24f7c"
      },
      "source": [
        "encoded_review = tokenizer.encode_plus(\n",
        "  review_text,\n",
        "  max_length=MAX_LEN,\n",
        "  add_special_tokens=True,\n",
        "  return_token_type_ids=False,\n",
        "  pad_to_max_length=True,\n",
        "  return_attention_mask=True,\n",
        "  return_tensors='pt',\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4l3lAEoiQ9E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2559dd8f-57a2-47d2-f665-9da0725481cd"
      },
      "source": [
        "input_ids = encoded_review['input_ids'].to(device)\n",
        "attention_mask = encoded_review['attention_mask'].to(device)\n",
        "\n",
        "output = model(input_ids, attention_mask)\n",
        "_, prediction = torch.max(output, dim=1)\n",
        "\n",
        "print(f'Review text: {review_text}')\n",
        "print(f'Sentiment  : {class_names[prediction]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review text: I love completing my Instagram! Best app ever!!!\n",
            "Sentiment  : positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKNegQA1Wj8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a758620-f96d-4e05-fdb3-afabe3dd8e18"
      },
      "source": [
        "torch.max(x,dim=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.max(values=tensor([0.9761, 0.9200, 0.8719, 0.4789, 0.9577, 0.7748, 0.5528, 0.7605, 0.9762,\n",
              "        0.8692, 0.6647, 0.8308, 0.9088, 0.8666, 0.8504, 0.9521]), indices=tensor([2, 0, 0, 2, 2, 0, 2, 1, 0, 1, 2, 0, 2, 1, 2, 0]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJJ8egHZWpjV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "59bd70fa-0b9c-4dcf-9016-e9ffff148ac3"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71UGmBW7d7Wz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "535f5b5b-33dd-4f50-cc78-e12e9bd13112"
      },
      "source": [
        "from transformers import BertModel\n",
        "model = BertModel.from_pretrained('bert-base-uncased',\n",
        "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
        "                                  )\n",
        "\n",
        "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LopQkpoucOjD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}